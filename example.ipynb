{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046d4bf0",
   "metadata": {},
   "source": [
    "# Model Auditor Example Notebook\n",
    "\n",
    "This notebook demonstrates the core functionality of the `model-auditor` package for evaluating ML model performance across subgroups with stratified metrics and bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and create a synthetic dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_auditor import Auditor\n",
    "from model_auditor.metrics import (\n",
    "    Sensitivity, Specificity, Precision, Recall,\n",
    "    F1Score, AUROC, AUPRC, MatthewsCorrelationCoefficient,\n",
    "    FBetaScore, TPR, FPR, nData, nPositive, nNegative\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "## Create Synthetic Dataset\n",
    "\n",
    "We'll create a synthetic medical dataset simulating a disease prediction model with demographic features for stratified evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "\n",
    "# Create demographic features\n",
    "age_groups = np.random.choice(['18-30', '31-50', '51-70', '70+'], n_samples, p=[0.2, 0.35, 0.30, 0.15])\n",
    "gender = np.random.choice(['Male', 'Female'], n_samples, p=[0.48, 0.52])\n",
    "region = np.random.choice(['North', 'South', 'East', 'West'], n_samples, p=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "# Create ground truth labels (disease status) with some demographic variation\n",
    "base_prevalence = 0.3\n",
    "disease_prob = np.where(age_groups == '70+', base_prevalence + 0.15,\n",
    "               np.where(age_groups == '51-70', base_prevalence + 0.08,\n",
    "               np.where(age_groups == '31-50', base_prevalence,\n",
    "                        base_prevalence - 0.05)))\n",
    "disease_status = np.random.binomial(1, disease_prob)\n",
    "\n",
    "# Create model prediction scores (continuous 0-1)\n",
    "# Model has varying performance across groups\n",
    "noise = np.random.normal(0, 0.15, n_samples)\n",
    "risk_score = np.clip(disease_status * 0.6 + (1 - disease_status) * 0.3 + noise, 0, 1)\n",
    "\n",
    "# Add some systematic bias for demonstration\n",
    "risk_score = np.where(age_groups == '70+', risk_score + 0.05, risk_score)\n",
    "risk_score = np.clip(risk_score, 0, 1)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'patient_id': range(n_samples),\n",
    "    'age_group': age_groups,\n",
    "    'gender': gender,\n",
    "    'region': region,\n",
    "    'risk_score': risk_score,\n",
    "    'disease_status': disease_status\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nDisease prevalence: {df['disease_status'].mean():.1%}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "## Basic Usage: Evaluating Model Performance\n",
    "\n",
    "Let's set up the Auditor to evaluate our model's performance across different demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7h8i9j0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Auditor\n",
    "auditor = Auditor()\n",
    "\n",
    "# Add the dataset\n",
    "auditor.add_data(df)\n",
    "\n",
    "# Add stratification features\n",
    "auditor.add_feature(name='age_group', label='Age Group')\n",
    "auditor.add_feature(name='gender', label='Gender')\n",
    "auditor.add_feature(name='region', label='Region')\n",
    "\n",
    "# Add the prediction score column with a threshold\n",
    "auditor.add_score(name='risk_score', label='Risk Score', threshold=0.5)\n",
    "\n",
    "# Add the outcome (ground truth) column\n",
    "auditor.add_outcome(name='disease_status')\n",
    "\n",
    "# Set metrics to evaluate\n",
    "auditor.set_metrics([\n",
    "    nData(),           # Sample size\n",
    "    nPositive(),       # Number of positive cases\n",
    "    Sensitivity(),     # True positive rate\n",
    "    Specificity(),     # True negative rate\n",
    "    Precision(),       # Positive predictive value\n",
    "    F1Score(),         # Harmonic mean of precision and recall\n",
    "    AUROC(),           # Area under ROC curve\n",
    "])\n",
    "\n",
    "print(\"Auditor configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h8i9j0k1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with bootstrap confidence intervals\n",
    "# Using 500 bootstraps for faster execution (use 1000+ for production)\n",
    "results = auditor.evaluate(score_name='risk_score', n_bootstraps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9j0k1l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all results as a DataFrame\n",
    "results_df = results.to_dataframe(n_decimals=3, metric_labels=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m3",
   "metadata": {},
   "source": [
    "## Accessing Specific Results\n",
    "\n",
    "You can access results at different levels of the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1l2m3n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for a specific feature\n",
    "print(\"=== Age Group Results ===\")\n",
    "age_results = results.features['age_group'].to_dataframe(n_decimals=3, metric_labels=True)\n",
    "display(age_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for a specific level within a feature\n",
    "print(\"=== Results for Age Group 70+ ===\")\n",
    "elderly_results = results.features['age_group'].levels['70+'].to_dataframe(n_decimals=3, metric_labels=True)\n",
    "display(elderly_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "## Threshold Optimization\n",
    "\n",
    "The Auditor can find the optimal decision threshold using the Youden index (maximizes sensitivity + specificity - 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n4o5p6q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold\n",
    "optimal_threshold = auditor.optimize_score_threshold(score_name='risk_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5p6q7r8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate with optimal threshold\n",
    "results_optimal = auditor.evaluate(\n",
    "    score_name='risk_score', \n",
    "    threshold=optimal_threshold,\n",
    "    n_bootstraps=500\n",
    ")\n",
    "\n",
    "print(f\"\\nResults with optimal threshold ({optimal_threshold:.3f}):\")\n",
    "results_optimal.features['overall'].to_dataframe(n_decimals=3, metric_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s9",
   "metadata": {},
   "source": [
    "## Fast Evaluation (Without Confidence Intervals)\n",
    "\n",
    "For quick exploration, you can disable bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast evaluation without CIs\n",
    "results_fast = auditor.evaluate(score_name='risk_score', n_bootstraps=None)\n",
    "results_fast.to_dataframe(n_decimals=3, metric_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8s9t0u1",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "\n",
    "You can create custom metrics by implementing the `AuditorMetric` protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9t0u1v2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_auditor.metrics import AuditorMetric\n",
    "\n",
    "class Accuracy(AuditorMetric):\n",
    "    \"\"\"Custom accuracy metric.\"\"\"\n",
    "    name = \"accuracy\"\n",
    "    label = \"Accuracy\"\n",
    "    inputs = [\"tp\", \"tn\", \"fp\", \"fn\"]\n",
    "    ci_eligible = True\n",
    "    \n",
    "    def data_call(self, data: pd.DataFrame) -> float:\n",
    "        tp = data[\"tp\"].sum()\n",
    "        tn = data[\"tn\"].sum()\n",
    "        fp = data[\"fp\"].sum()\n",
    "        fn = data[\"fn\"].sum()\n",
    "        return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "\n",
    "class BalancedAccuracy(AuditorMetric):\n",
    "    \"\"\"Balanced accuracy (average of sensitivity and specificity).\"\"\"\n",
    "    name = \"balanced_accuracy\"\n",
    "    label = \"Balanced Accuracy\"\n",
    "    inputs = [\"tp\", \"tn\", \"fp\", \"fn\"]\n",
    "    ci_eligible = True\n",
    "    \n",
    "    def data_call(self, data: pd.DataFrame, eps: float = 1e-8) -> float:\n",
    "        tp = data[\"tp\"].sum()\n",
    "        tn = data[\"tn\"].sum()\n",
    "        fp = data[\"fp\"].sum()\n",
    "        fn = data[\"fn\"].sum()\n",
    "        sensitivity = tp / (tp + fn + eps)\n",
    "        specificity = tn / (tn + fp + eps)\n",
    "        return (sensitivity + specificity) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t0u1v2w3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use custom metrics\n",
    "auditor.set_metrics([\n",
    "    nData(),\n",
    "    Accuracy(),\n",
    "    BalancedAccuracy(),\n",
    "    Sensitivity(),\n",
    "    Specificity(),\n",
    "    MatthewsCorrelationCoefficient(),\n",
    "])\n",
    "\n",
    "results_custom = auditor.evaluate(score_name='risk_score', n_bootstraps=500)\n",
    "results_custom.features['age_group'].to_dataframe(n_decimals=3, metric_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1v2w3x4",
   "metadata": {},
   "source": [
    "## Using F-beta Score\n",
    "\n",
    "The F-beta score allows you to weight precision vs recall. Beta < 1 favors precision, beta > 1 favors recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2w3x4y5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different F-beta scores\n",
    "auditor.set_metrics([\n",
    "    nData(),\n",
    "    Precision(),\n",
    "    Recall(),\n",
    "    FBetaScore(beta=0.5),   # Weights precision higher\n",
    "    F1Score(),              # Equal weight (beta=1)\n",
    "    FBetaScore(beta=2.0),   # Weights recall higher\n",
    "])\n",
    "\n",
    "results_fbeta = auditor.evaluate(score_name='risk_score', n_bootstraps=None)\n",
    "results_fbeta.features['overall'].to_dataframe(n_decimals=3, metric_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3x4y5z6",
   "metadata": {},
   "source": [
    "## Hierarchical Visualization\n",
    "\n",
    "The `HierarchyPlotter` creates data structures for sunburst or treemap visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x4y5z6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_auditor.plotting import HierarchyPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the hierarchy plotter\n",
    "plotter = HierarchyPlotter()\n",
    "plotter.set_data(df)\n",
    "plotter.set_features(['region', 'age_group', 'gender'])  # Hierarchy order\n",
    "plotter.set_score(name='risk_score')\n",
    "plotter.set_aggregator('mean')  # Aggregate scores by mean\n",
    "\n",
    "# Compile the plot data\n",
    "plot_data = plotter.compile(container='All Patients')\n",
    "\n",
    "print(f\"Number of nodes: {len(plot_data.labels)}\")\n",
    "print(f\"\\nFirst 10 labels: {plot_data.labels[:10]}\")\n",
    "print(f\"First 10 values: {plot_data.values[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sunburst visualization with Plotly (if available)\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    fig = go.Figure(go.Sunburst(\n",
    "        labels=plot_data.labels,\n",
    "        ids=plot_data.ids,\n",
    "        parents=plot_data.parents,\n",
    "        values=plot_data.values,\n",
    "        marker=dict(\n",
    "            colors=plot_data.colors,\n",
    "            colorscale='RdYlGn',\n",
    "            cmid=0.5\n",
    "        ),\n",
    "        branchvalues='total',\n",
    "        hovertemplate='<b>%{label}</b><br>Count: %{value}<br>Mean Score: %{color:.3f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Risk Score Distribution by Demographics',\n",
    "        width=800,\n",
    "        height=800\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "except ImportError:\n",
    "    print(\"Plotly not installed. Install with: pip install plotly\")\n",
    "    print(\"\\nPlot data is available in plot_data object for use with other visualization libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## Custom Hierarchies\n",
    "\n",
    "For more complex visualizations, you can define custom hierarchies with conditional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_auditor.plotting.schemas import Hierarchy, HLevel, HItem\n",
    "\n",
    "# Create a custom hierarchy\n",
    "custom_hierarchy = Hierarchy(levels=[\n",
    "    HLevel([HItem(name='gender')]),           # First level: Gender\n",
    "    HLevel([HItem(name='age_group')]),        # Second level: Age Group\n",
    "])\n",
    "\n",
    "plotter2 = HierarchyPlotter()\n",
    "plotter2.set_data(df)\n",
    "plotter2.set_features(custom_hierarchy)\n",
    "plotter2.set_score(name='risk_score')\n",
    "plotter2.set_aggregator('median')  # Use median instead of mean\n",
    "\n",
    "plot_data2 = plotter2.compile(container='Population')\n",
    "\n",
    "# Show the hierarchy structure\n",
    "hierarchy_df = pd.DataFrame({\n",
    "    'Label': plot_data2.labels,\n",
    "    'Parent': plot_data2.parents,\n",
    "    'Count': plot_data2.values,\n",
    "    'Median Score': [f\"{c:.3f}\" for c in plot_data2.colors]\n",
    "})\n",
    "hierarchy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## Complete Workflow Example\n",
    "\n",
    "Here's a complete example putting everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2g3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow\n",
    "from model_auditor import Auditor\n",
    "from model_auditor.metrics import (\n",
    "    Sensitivity, Specificity, AUROC, F1Score, \n",
    "    nData, nPositive, nNegative\n",
    ")\n",
    "\n",
    "# 1. Initialize and configure\n",
    "auditor = Auditor()\n",
    "auditor.add_data(df)\n",
    "auditor.add_feature(name='age_group', label='Age Group')\n",
    "auditor.add_feature(name='gender', label='Gender')\n",
    "auditor.add_score(name='risk_score', label='Risk Score')\n",
    "auditor.add_outcome(name='disease_status')\n",
    "\n",
    "# 2. Find optimal threshold\n",
    "threshold = auditor.optimize_score_threshold(score_name='risk_score')\n",
    "\n",
    "# 3. Set comprehensive metrics\n",
    "auditor.set_metrics([\n",
    "    nData(),\n",
    "    nPositive(),\n",
    "    nNegative(),\n",
    "    Sensitivity(),\n",
    "    Specificity(),\n",
    "    F1Score(),\n",
    "    AUROC(),\n",
    "])\n",
    "\n",
    "# 4. Evaluate with confidence intervals\n",
    "results = auditor.evaluate(\n",
    "    score_name='risk_score',\n",
    "    threshold=threshold,\n",
    "    n_bootstraps=500\n",
    ")\n",
    "\n",
    "# 5. Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOptimal Threshold: {threshold:.3f}\")\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(f\"Disease Prevalence: {df['disease_status'].mean():.1%}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"OVERALL PERFORMANCE\")\n",
    "print(\"-\"*60)\n",
    "display(results.features['overall'].to_dataframe(n_decimals=3, metric_labels=True))\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PERFORMANCE BY AGE GROUP\")\n",
    "print(\"-\"*60)\n",
    "display(results.features['age_group'].to_dataframe(n_decimals=3, metric_labels=True))\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"PERFORMANCE BY GENDER\")\n",
    "print(\"-\"*60)\n",
    "display(results.features['gender'].to_dataframe(n_decimals=3, metric_labels=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Basic Usage**: Setting up the Auditor with data, features, scores, and outcomes\n",
    "2. **Metric Evaluation**: Computing stratified metrics with bootstrap confidence intervals\n",
    "3. **Threshold Optimization**: Finding optimal decision thresholds using the Youden index\n",
    "4. **Custom Metrics**: Creating and using custom metric implementations\n",
    "5. **F-beta Scores**: Adjusting precision/recall trade-offs\n",
    "6. **Hierarchical Visualization**: Creating data for sunburst/treemap plots\n",
    "7. **Complete Workflow**: Putting it all together for a comprehensive model audit\n",
    "\n",
    "For more information, see the [README](README.md) or the module docstrings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
